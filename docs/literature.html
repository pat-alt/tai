<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang=""><head>
  <meta charset="utf-8">
  <meta name="generator" content="quarto-0.2.349">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
  <title>Trustworthy AI - Literature</title>
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    span.underline{text-decoration: underline;}
    div.column{display: inline-block; vertical-align: top; width: 50%;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    div.csl-bib-body { }
    div.csl-entry {
      clear: both;
    }
    .hanging div.csl-entry {
      margin-left:2em;
      text-indent:-2em;
    }
    div.csl-left-margin {
      min-width:2em;
      float:left;
    }
    div.csl-right-inline {
      margin-left:2em;
      padding-left:1em;
    }
    div.csl-indent {
      margin-left: 2em;
    }
  </style>

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
  <script src="site_libs/quarto-nav/quarto-nav.js"></script>
  <script src="site_libs/quarto-nav/headroom.min.js"></script>
  <script src="site_libs/clipboard/clipboard.min.js"></script>
  <meta name="quarto:offset" content="./">
  <script src="site_libs/quarto-search/autocomplete.umd.js"></script>
  <script src="site_libs/quarto-search/fuse.min.js"></script>
  <script src="site_libs/quarto-search/quarto-search.js"></script>
  <script src="site_libs/quarto-html/quarto.js"></script>
  <script src="site_libs/quarto-html/popper.min.js"></script>
  <script src="site_libs/quarto-html/tippy.umd.min.js"></script>
  <script src="site_libs/quarto-html/anchor.min.js"></script>
  <link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
  <link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet">
  <script src="site_libs/bootstrap/bootstrap.min.js"></script>
  <link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
  <link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet">
  <script id="quarto-search-options" type="application/json">{
    "location": "navbar",
    "copy-button": false,
    "collapse-after": 2,
    "panel-placement": "end",
    "type": "overlay",
    "limit": 20,
    "language": {
      "search-no-results-text": "No results",
      "search-matching-documents-text": "matching documents",
      "search-copy-link-title": "Copy link to search",
      "search-hide-matches-text": "Hide additional matches",
      "search-more-match-text": "more match in this document",
      "search-more-matches-text": "more matches in this document",
      "search-clear-button-title": "Clear",
      "search-detached-cancel-button-title": "Cancel",
      "search-submit-button-title": "Submit"
    }
  }</script>
  <link rel="stylesheet" href="styles.css">
</head>
<body>
<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-light ">
      <div class="navbar-container container-fluid">
      <a class="navbar-brand" href="./index.html">
    <span class="navbar-title">Trustworthy AI</span>
  </a>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="./index.html">Home</a>
  </li>  
  <li class="nav-item">
    <a class="nav-link active" href="./literature.html" aria-current="page">Literature</a>
  </li>  
</ul>
              <div id="quarto-search" class="" title="Search"></div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
      <nav id="TOC" role="doc-toc">
<h2 id="toc-title">On this page</h2>
<ul>
<li><a href="#explainability" class="nav-link active" data-scroll-target="#explainability">Explainability</a>
<ul class="collapse">
<li><a href="#surrogate-explainers" class="nav-link" data-scroll-target="#surrogate-explainers">Surrogate Explainers</a></li>
<li><a href="#criticism-surrogate-explainers" class="nav-link" data-scroll-target="#criticism-surrogate-explainers">Criticism (Surrogate Explainers)</a></li>
<li><a href="#counterfactual-explanations-ce" class="nav-link" data-scroll-target="#counterfactual-explanations-ce">Counterfactual Explanations (CE)</a></li>
</ul></li>
<li><a href="#bayesian" class="nav-link" data-scroll-target="#bayesian">Bayesian Deep Learning</a>
<ul class="collapse">
<li><a href="#background" class="nav-link" data-scroll-target="#background">Background</a></li>
<li><a href="#interpretability" class="nav-link" data-scroll-target="#interpretability">Interpretability</a></li>
<li><a href="#uncertainty-quantification-and-applications" class="nav-link" data-scroll-target="#uncertainty-quantification-and-applications">Uncertainty quantification and applications</a></li>
<li><a href="#computational-efficiency" class="nav-link" data-scroll-target="#computational-efficiency">Computational efficiency</a></li>
</ul></li>
<li><a href="#causal-ai" class="nav-link" data-scroll-target="#causal-ai">Causal AI</a>
<ul class="collapse">
<li><a href="#background-1" class="nav-link" data-scroll-target="#background-1">Background</a></li>
<li><a href="#structure-learning" class="nav-link" data-scroll-target="#structure-learning">Structure learning</a></li>
<li><a href="#causal-ce" class="nav-link" data-scroll-target="#causal-ce">Link to CE and algorithmic recourse</a></li>
</ul></li>
<li><a href="#robustness" class="nav-link" data-scroll-target="#robustness">Robustness</a>
<ul class="collapse">
<li><a href="#background-2" class="nav-link" data-scroll-target="#background-2">Background</a></li>
<li><a href="#thoughts" class="nav-link" data-scroll-target="#thoughts">Thoughts</a></li>
</ul></li>
<li><a href="#applications" class="nav-link" data-scroll-target="#applications">Applications</a></li>
<li><a href="#references" class="nav-link" data-scroll-target="#references">References</a></li>
</ul>
<div class="toc-actions"><div><i class="bi bi-github"></i></div><div class="action-links"><p><a href="https://github.com/pat-alt/tai/issues/new" class="toc-action">Report an issue</a></p></div></div></nav>
    </div>
<!-- main -->
<main class="content">
<header id="title-block-header">
<h1 class="title display-7">Literature</h1>
</header>

<p>This is a collection of interesting papers and thoughts around Trustworthy AI that I have been gradually compiling during the early stages of my PhD. Descriptions of papers are very brief. If you’d like to have access to more detailed handwritten notes, please just drop me a <a href="mailto:%20p.altmeyer@tudelft.nl">line</a>. A list of all references linked here can be found at the bottom.</p>
<section id="explainability" class="level2">
<h2 class="anchored" data-anchor-id="explainability">Explainability</h2>
<section id="surrogate-explainers" class="level3">
<h3 class="anchored" data-anchor-id="surrogate-explainers">Surrogate Explainers</h3>
<ul>
<li><span class="citation" data-cites="ribeiro2016should">Ribeiro, Singh, and Guestrin (<a href="#ref-ribeiro2016should" role="doc-biblioref">2016</a>)</span> propose Local Interpretable Model-Agnostic Explanations (LIME): the approach involves generating local perturbations in the input space, deriving predictions from the original classifier and than fitting a white box model (e.g.&nbsp;linear regression) on this synthetic data set.</li>
<li><span class="citation" data-cites="lundberg2017unified">Lundberg and Lee (<a href="#ref-lundberg2017unified" role="doc-biblioref">2017</a>)</span> propose SHAP as a provably unified approach to additive feature attribution methods (including LIME) with certain desiderata. Contrary to LIME, this approach involves permuting through the feature space and checking how different features impact model predictions when they are included in the permutations.</li>
</ul>
</section>
<section id="criticism-surrogate-explainers" class="level3">
<h3 class="anchored" data-anchor-id="criticism-surrogate-explainers">Criticism (Surrogate Explainers)</h3>
<blockquote class="blockquote">
<p>“Explanatory models by definition do not produce 100% reliable explanations, because they are approximations. This means explanations can’t be fully trusted, and so neither can the original model.” – <a href="https://www.causalens.com/blog/xai-doesnt-explain/">causaLens</a>, 2021</p>
</blockquote>
<ul>
<li><span class="citation" data-cites="mittelstadt2019explaining">Mittelstadt, Russell, and Wachter (<a href="#ref-mittelstadt2019explaining" role="doc-biblioref">2019</a>)</span> points out that there is a gap in the understanding of what explanations are between computer scientists and explanation scientists (social scientists, cognitive scientists, pyschologists, …). Current methods produce at best locally reliable explanations. There needs to be shift towards <em>interactive</em> explanations.</li>
<li><span class="citation" data-cites="rudin2019stop">Rudin (<a href="#ref-rudin2019stop" role="doc-biblioref">2019</a>)</span> argues that instead of bothering with explanations for black box models we should focus on designing inherently interpretable models. In her view the trade-off between (intrinsic) explainability and performance is not as clear-cut as people claim.</li>
<li><span class="citation" data-cites="lakkaraju2020fool">Lakkaraju and Bastani (<a href="#ref-lakkaraju2020fool" role="doc-biblioref">2020</a>)</span> show how misleading black box explanations can manipulate users into trusting an untrustworthy model.</li>
<li><span class="citation" data-cites="slack2020fooling">Slack et al. (<a href="#ref-slack2020fooling" role="doc-biblioref">2020</a>)</span> demonstrate that both LIME and SHAP are not reliable: their reliance on feature perturbations makes them susceptible to adversarial attacks.</li>
</ul>
<div class="thoughts">
<ul>
<li>Can we quantify robustness of surrogate explainers?</li>
<li>Comparison of different complexity measures.</li>
<li>Design surrogate explainer that incorporates causality.</li>
<li>Surrogate explainers by definition are approximations of black box models, so can never be 100% trusted.</li>
<li>Adversarially robust surrogate explainers by minimizing divergence between observed and perturbed data. <span class="citation" data-cites="slack2020fooling">(<a href="#ref-slack2020fooling" role="doc-biblioref">Slack et al. 2020</a>)</span></li>
</ul>
</div>
</section>
<section id="counterfactual-explanations-ce" class="level3">
<h3 class="anchored" data-anchor-id="counterfactual-explanations-ce">Counterfactual Explanations (CE)</h3>
<ul>
<li><span class="citation" data-cites="wachter2017counterfactual">Wachter, Mittelstadt, and Russell (<a href="#ref-wachter2017counterfactual" role="doc-biblioref">2017</a>)</span> were among the first to propose counterfactual explanations that do not require knowledge about the inner workings of a black box model.</li>
<li><span class="citation" data-cites="ustun2019actionable">Ustun, Spangher, and Liu (<a href="#ref-ustun2019actionable" role="doc-biblioref">2019</a>)</span> propose a framework for <em>actionable</em> recourse in the context of linear classifiers.</li>
<li><span class="citation" data-cites="joshi2019towards">Joshi et al. (<a href="#ref-joshi2019towards" role="doc-biblioref">2019</a>)</span> extend the framework of <span class="citation" data-cites="ustun2019actionable">Ustun, Spangher, and Liu (<a href="#ref-ustun2019actionable" role="doc-biblioref">2019</a>)</span>. Their proposed REVISE method is applicable to a broader class of models including black box classifiers and structural causal models. For a summary see <a href="https://pat-alt.github.io/2021/04/27/individual-recourse-for-black-box-models/">here</a> and for a set of slides see <a href="https://pat-alt.github.io/2021/04/27/individual-recourse-for-black-box-models/paper_presentation.pdf">here</a>.</li>
<li><span class="citation" data-cites="poyiadzi2020face">Poyiadzi et al. (<a href="#ref-poyiadzi2020face" role="doc-biblioref">2020</a>)</span> propose FACE: feasible and actionable counterfactual explanations. The premise is that the shortest distance to the decision boundary may not be a desirable counterfactual.</li>
<li><span class="citation" data-cites="schut2021generating">Schut et al. (<a href="#ref-schut2021generating" role="doc-biblioref">2021</a>)</span> introduce Bayesian modelling to the context of CE: their approach implicitly minimizes <em>aleatoric</em> and <em>epistemic</em> uncertainty to generate a CE that us <em>unambiguous</em> and <em>realistic</em>, respectively.</li>
</ul>
<div class="thoughts">
<ul>
<li><strong>Test what counterfactual explanations are most desirable through user study at ING</strong></li>
<li>Design counterfactual explanations that incorporate causality
<ul>
<li><strong>There are in fact several very recent papers (including <span class="citation" data-cites="joshi2019towards">Joshi et al. (<a href="#ref-joshi2019towards" role="doc-biblioref">2019</a>)</span>) that link CGMs to counterfactual explanations</strong> (see below)</li>
</ul></li>
<li>Time series: what is the link to counterfactual analysis in multivariate time series? (e.g.&nbsp;chapter 4 in <span class="citation" data-cites="kilian2017structural">Kilian and Lütkepohl (<a href="#ref-kilian2017structural" role="doc-biblioref">2017</a>)</span>)</li>
<li>What about the continuous outcome variables? (e.g.&nbsp;target inflation rate, … ING cases?)</li>
<li>How do counterfactual explainers fare where LIME/SHAP fail? <span class="citation" data-cites="slack2020fooling">(<a href="#ref-slack2020fooling" role="doc-biblioref">Slack et al. 2020</a>)</span></li>
<li>Can counterfactual explainers be fooled much like LIME/SHAP?</li>
<li>Can we establish a link between counterfactual and surrogate explainers? Important attributes identified by LIME/SHAP should play a prominent role in counterfactuals.</li>
<li>Can counterfactual explainers be used to detect adversarial examples?</li>
<li>Limiting behaviour: what happens if all individual with negative outcome move across the original decision boundary?</li>
</ul>
</div>
</section>
</section>
<section id="bayesian" class="level2">
<h2 class="anchored" data-anchor-id="bayesian">Bayesian Deep Learning</h2>
<section id="background" class="level3">
<h3 class="anchored" data-anchor-id="background">Background</h3>
<ul>
<li><span class="citation" data-cites="jospin2020hands">Jospin et al. (<a href="#ref-jospin2020hands" role="doc-biblioref">2020</a>)</span> provide a detailed and hands-on introduction to Bayesian Deep Learning.</li>
<li><span class="citation" data-cites="murphy2022probabilistic">Murphy (<a href="#ref-murphy2022probabilistic" role="doc-biblioref">2022</a>)</span> is a text book that treats machine learning from a probabilistic perspective. It includes sections dedicated to deep learning.</li>
</ul>
</section>
<section id="interpretability" class="level3">
<h3 class="anchored" data-anchor-id="interpretability">Interpretability</h3>
<ul>
<li><span class="citation" data-cites="ish2019interpreting">Ish-Horowicz et al. (<a href="#ref-ish2019interpreting" role="doc-biblioref">2019</a>)</span> proposes an entropy-based measure for interpreting Bayesian Neural Networks. For a summary see <a href="https://pat-alt.github.io/2021/02/07/a-peek-inside-the-black-box-interpreting-neural-networks/">here</a>.</li>
</ul>
</section>
<section id="uncertainty-quantification-and-applications" class="level3">
<h3 class="anchored" data-anchor-id="uncertainty-quantification-and-applications">Uncertainty quantification and applications</h3>
<ul>
<li><span class="citation" data-cites="gal2016dropout">Gal and Ghahramani (<a href="#ref-gal2016dropout" role="doc-biblioref">2016</a>)</span> demonstrate that a dropout neural network is equivalent to approximate inference in Bayesian modelling of deep Gaussian processes. This makes it straight-forward to quantify uncertainty in deep learning through simple Monte-Carlo methods.</li>
<li><span class="citation" data-cites="gal2017deep">Gal, Islam, and Ghahramani (<a href="#ref-gal2017deep" role="doc-biblioref">2017</a>)</span> propose a way towards deep active Bayesian learning that plays with the ideas of aleatoric and epsitemic uncertainty: a structured approach to human-in-the-loop deep learning that can work with small data sets.
<ul>
<li><span class="citation" data-cites="kirsch2019batchbald">Kirsch, Van Amersfoort, and Gal (<a href="#ref-kirsch2019batchbald" role="doc-biblioref">2019</a>)</span> extend these ideas.</li>
</ul></li>
</ul>
</section>
<section id="computational-efficiency" class="level3">
<h3 class="anchored" data-anchor-id="computational-efficiency">Computational efficiency</h3>
<ul>
<li>Quantum computing likely to make probabilistic modelling more computationally efficient. <span class="citation" data-cites="kehoe2021defence">Kehoe et al. (<a href="#ref-kehoe2021defence" role="doc-biblioref">2021</a>)</span> propose a Bayesian approach to DL using quantum processors that promises to be more robust than conventional DNNs.</li>
<li>Using simple concentration inequalities Maxim Panov proposes a measure for total uncertainty of Deep Neural Networks (no numerical methods needed) – missing a paper references here.</li>
</ul>
<div class="thoughts">
<ul>
<li><strong>Compare explainability in Bayesian setting (e.g.&nbsp;RATE <span class="citation" data-cites="ish2019interpreting">(<a href="#ref-ish2019interpreting" role="doc-biblioref">Ish-Horowicz et al. 2019</a>)</span>) to surrogate (and counterfactual) explainers? (ING models)</strong></li>
<li>Link to AFR track on quantum ML.</li>
<li>Link to uncertainty quantification for Deep Vector Autoregression <span class="citation" data-cites="agusti2021deep">(<a href="#ref-agusti2021deep" role="doc-biblioref"><strong>agusti2021deep?</strong></a>)</span>.</li>
</ul>
</div>
</section>
</section>
<section id="causal-ai" class="level2">
<h2 class="anchored" data-anchor-id="causal-ai">Causal AI</h2>
<section id="background-1" class="level3">
<h3 class="anchored" data-anchor-id="background-1">Background</h3>
<ul>
<li>There is an emerging view that that current efforts towards interpretability and robustness are fruitless and only an incorporation of causality can provide answers <span class="citation" data-cites="pearl2018book">(<a href="#ref-pearl2018book" role="doc-biblioref">Pearl and Mackenzie 2018</a>)</span>.</li>
<li><span class="citation" data-cites="pearl2019seven">Pearl (<a href="#ref-pearl2019seven" role="doc-biblioref">2019</a>)</span> argues that AI is current stuck at the <em>association</em> level: models are limited to learning <span class="math inline">\(P(y|X)\)</span> (“glorified curve fitting”). Starting from causal graphical models (CGM) improves transparency and domain adaptability.</li>
</ul>
</section>
<section id="structure-learning" class="level3">
<h3 class="anchored" data-anchor-id="structure-learning">Structure learning</h3>
<ul>
<li><span class="citation" data-cites="zheng2018dags">Zheng et al. (<a href="#ref-zheng2018dags" role="doc-biblioref">2018</a>)</span> proposes to cast the combinatorial problem of learning a CGM into a continuous problem that can be learned through standard non-convex constrained optimization for linear structural equation models (SEM).</li>
<li><span class="citation" data-cites="lachapelle2019gradient">Lachapelle et al. (<a href="#ref-lachapelle2019gradient" role="doc-biblioref">2019</a>)</span> extend this idea to the non-linear case.</li>
<li><span class="citation" data-cites="bussmann2020neural">Bussmann, Nys, and Latré (<a href="#ref-bussmann2020neural" role="doc-biblioref">2020</a>)</span> propose Neural Additive Vector Autoregression (NAVAR) for (Granger) causal discovery in time series setting. The model can be seen as a Generalised Additive Model and is therefore inherently (somewhat) interpretable. It is based on the assumption that contemporary dependencies between variables are linear, only dependencies through time require non-linear model.</li>
</ul>
</section>
<section id="causal-ce" class="level3">
<h3 class="anchored" data-anchor-id="causal-ce">Link to CE and algorithmic recourse</h3>
<ul>
<li><span class="citation" data-cites="joshi2019towards">Joshi et al. (<a href="#ref-joshi2019towards" role="doc-biblioref">2019</a>)</span> make an interesting link between CGM and counterfactual explanations: they draw an analogy between hidden confounders in CGMs and the latent manifold which REVISE traverses to propose recourse. Run a single experiment on TWIN dataset and show that recommended recourse changes qualitatively as confounding is introduced.</li>
<li><span class="citation" data-cites="karimi2020algorithmic">Karimi et al. (<a href="#ref-karimi2020algorithmic" role="doc-biblioref">2020</a>)</span> develop two probabilistic approaches to algorithmic recourse in the case of limited causal knowledge.
<ul>
<li>In essence the probabilistic approach boils down to assuming a Gaussian Process prior for the causal mapping parent nodes to node <span class="math inline">\(X\)</span>. This yields a <em>posterior noise distribution</em>, which in turn can be used to draw from a <em>counterfactual distribution</em>.</li>
</ul></li>
<li><span class="citation" data-cites="karimi2021algorithmic">Karimi, Schölkopf, and Valera (<a href="#ref-karimi2021algorithmic" role="doc-biblioref">2021</a>)</span> demonstrate how to go from counterfactuals to interventions in the case of complete knowledge of the CGM. Propose a shift of paradigm from <em>recourse via counterfactuals</em> to <em>recourse through minimal interventions</em>.</li>
</ul>
<div class="thoughts">
<ul>
<li>Can explore the link between CGM and CE further, perhaps in the context of Bayesian classifier <span class="citation" data-cites="schut2021generating">(<a href="#ref-schut2021generating" role="doc-biblioref">Schut et al. 2021</a>)</span>.
<ul>
<li><strong>In particular, it might be possible to draw an analogue between <span class="citation" data-cites="schut2021generating">Schut et al. (<a href="#ref-schut2021generating" role="doc-biblioref">2021</a>)</span> (low epistemic + aleatoric uncertainty) and the counterfactual distribution proposed by <span class="citation" data-cites="karimi2020algorithmic">Karimi et al. (<a href="#ref-karimi2020algorithmic" role="doc-biblioref">2020</a>)</span>. Could further try to account for hidden confounders as in <span class="citation" data-cites="joshi2019towards">Joshi et al. (<a href="#ref-joshi2019towards" role="doc-biblioref">2019</a>)</span>.</strong></li>
</ul></li>
<li><span class="citation" data-cites="karimi2021algorithmic">Karimi, Schölkopf, and Valera (<a href="#ref-karimi2021algorithmic" role="doc-biblioref">2021</a>)</span> can be solved through by building on existing frameworks for generating nearest counterfactual explanations - could try to apply <span class="citation" data-cites="schut2021generating">Schut et al. (<a href="#ref-schut2021generating" role="doc-biblioref">2021</a>)</span>?</li>
<li><strong>Applications at ING?</strong>
<ul>
<li>Apply to loan application decision system (if exists)</li>
<li>Apply to credit scoring (perhaps even Dutch government scandal)</li>
<li>…</li>
</ul></li>
</ul>
</div>
</section>
</section>
<section id="robustness" class="level2">
<h2 class="anchored" data-anchor-id="robustness">Robustness</h2>
<section id="background-2" class="level3">
<h3 class="anchored" data-anchor-id="background-2">Background</h3>
<ul>
<li><span class="citation" data-cites="szegedy2013intriguing">Szegedy et al. (<a href="#ref-szegedy2013intriguing" role="doc-biblioref">2013</a>)</span> were the first to point out the existence of adversarial examples in the image classification domain.</li>
<li><span class="citation" data-cites="goodfellow2014explaining">Goodfellow, Shlens, and Szegedy (<a href="#ref-goodfellow2014explaining" role="doc-biblioref">2014</a>)</span> argue that the existence of adversarial examples can be explained solely by the locally-linear nature of artificial neural networks. They show how simple linear perturbation through their <em>fast gradient sign method</em> can consistently fool many state-of-the-art neural networks. Adversarial training can improve robustness to some extent, but DNNs are still highly confident with respect to misclassified labels.</li>
<li><span class="citation" data-cites="carlini2017towards">Carlini and Wagner (<a href="#ref-carlini2017towards" role="doc-biblioref">2017</a>)</span> show that an initially promising method for robustifying DNNs, namely <em>defensive distillation</em>, is in fact insufficient. They argue that their adversarial attacks should serve as a benchmark for evaluating the robustness of DNNs.</li>
</ul>
</section>
<section id="thoughts" class="level3 thoughts">
<h3 class="thoughts anchored" data-anchor-id="thoughts">Thoughts</h3>
<div class="thoughts">
<ul>
<li><strong>Link to anomaly detection (ING)</strong></li>
<li><strong>Out-of-distribution detection for time series models (e.g.&nbsp;avoid Covid scenarios leading to model failures <span class="citation" data-cites="bholat2020impact">(<a href="#ref-bholat2020impact" role="doc-biblioref">Bholat, Gharbawi, and Thew 2020</a>)</span>)</strong>.</li>
<li>If adversarial training affects the success of adversarial attacks, does it also affect success of CE?</li>
<li>Can we penalize instability much like we penalize complexity in empirical risk minimization?</li>
</ul>
</div>
</section>
</section>
<section id="applications" class="level2">
<h2 class="anchored" data-anchor-id="applications">Applications</h2>
<ul>
<li><strong>Financial Stability</strong>:
<ul>
<li>AI robustness and the Covid crisis - negative impact of ML on model performance.</li>
<li>Potential for herding behaviour if large share of market participants uses off-the-shelf ML tools <span class="citation" data-cites="oecd2021artificial">(<a href="#ref-oecd2021artificial" role="doc-biblioref">OECD 2021</a>)</span>.</li>
</ul></li>
<li><strong>Market Microstructure</strong>:
<ul>
<li>ML model collusion hard to detect <span class="citation" data-cites="oecd2021artificial">(<a href="#ref-oecd2021artificial" role="doc-biblioref">OECD 2021</a>)</span>.</li>
<li>Lack of explainability inhibits timely model adjustments <span class="citation" data-cites="oecd2021artificial">(<a href="#ref-oecd2021artificial" role="doc-biblioref">OECD 2021</a>)</span>.</li>
<li>Intentional lack of transparency for proprietary trading <span class="citation" data-cites="oecd2021artificial">(<a href="#ref-oecd2021artificial" role="doc-biblioref">OECD 2021</a>)</span>.</li>
</ul></li>
<li><strong>SupTech and RegTech</strong>:
<ul>
<li>Using AI to self-regulate in a transparent, trustworthy way <span class="citation" data-cites="oecd2021artificial">(<a href="#ref-oecd2021artificial" role="doc-biblioref">OECD 2021</a>)</span>.</li>
</ul></li>
<li><strong>Monetary policy</strong>:
<ul>
<li>In <span class="citation" data-cites="altmeyer2021deep">Altmeyer, Agusti, and Vidal-Quadras Costa (<a href="#ref-altmeyer2021deep" role="doc-biblioref">2021</a>)</span> we show how to incorporate deep learning in the context of Vector Autoregression for macroeconomic data.</li>
</ul></li>
</ul>
</section>
<section id="references" class="level1 unnumbered">
<h1 class="unnumbered">References</h1>
<div id="refs" class="references csl-bib-body hanging-indent" role="doc-bibliography">
<div id="ref-altmeyer2021deep" class="csl-entry" role="doc-biblioentry">
Altmeyer, Patrick, Marc Agusti, and Ignacio Vidal-Quadras Costa. 2021. <span>“Deep Vector Autoregression for Macroeconomic Data.”</span> <a href="https://thevoice.bse.eu/wp-content/uploads/2021/07/ds21-project-agusti-et-al.pdf">https://thevoice.bse.eu/wp-content/uploads/2021/07/ds21-project-agusti-et-al.pdf</a>.
</div>
<div id="ref-bholat2020impact" class="csl-entry" role="doc-biblioentry">
Bholat, D, M Gharbawi, and O Thew. 2020. <span>“The Impact of Covid on Machine Learning and Data Science in UK Banking.”</span> <em>Bank of England Quarterly Bulletin, Q4</em>.
</div>
<div id="ref-bussmann2020neural" class="csl-entry" role="doc-biblioentry">
Bussmann, Bart, Jannes Nys, and Steven Latré. 2020. <span>“Neural Additive Vector Autoregression Models for Causal Discovery in Time Series Data.”</span> <em>arXiv Preprint arXiv:2010.09429</em>.
</div>
<div id="ref-carlini2017towards" class="csl-entry" role="doc-biblioentry">
Carlini, Nicholas, and David Wagner. 2017. <span>“Towards Evaluating the Robustness of Neural Networks.”</span> In <em>2017 Ieee Symposium on Security and Privacy (Sp)</em>, 39–57. IEEE.
</div>
<div id="ref-gal2016dropout" class="csl-entry" role="doc-biblioentry">
Gal, Yarin, and Zoubin Ghahramani. 2016. <span>“Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning.”</span> In <em>International Conference on Machine Learning</em>, 1050–59. PMLR.
</div>
<div id="ref-gal2017deep" class="csl-entry" role="doc-biblioentry">
Gal, Yarin, Riashat Islam, and Zoubin Ghahramani. 2017. <span>“Deep Bayesian Active Learning with Image Data.”</span> In <em>International Conference on Machine Learning</em>, 1183–92. PMLR.
</div>
<div id="ref-goodfellow2014explaining" class="csl-entry" role="doc-biblioentry">
Goodfellow, Ian J, Jonathon Shlens, and Christian Szegedy. 2014. <span>“Explaining and Harnessing Adversarial Examples.”</span> <em>arXiv Preprint arXiv:1412.6572</em>.
</div>
<div id="ref-ish2019interpreting" class="csl-entry" role="doc-biblioentry">
Ish-Horowicz, Jonathan, Dana Udwin, Seth Flaxman, Sarah Filippi, and Lorin Crawford. 2019. <span>“Interpreting Deep Neural Networks Through Variable Importance.”</span> <em>arXiv Preprint arXiv:1901.09839</em>.
</div>
<div id="ref-joshi2019towards" class="csl-entry" role="doc-biblioentry">
Joshi, Shalmali, Oluwasanmi Koyejo, Warut Vijitbenjaronk, Been Kim, and Joydeep Ghosh. 2019. <span>“Towards Realistic Individual Recourse and Actionable Explanations in Black-Box Decision Making Systems.”</span> <em>arXiv Preprint arXiv:1907.09615</em>.
</div>
<div id="ref-jospin2020hands" class="csl-entry" role="doc-biblioentry">
Jospin, Laurent Valentin, Wray Buntine, Farid Boussaid, Hamid Laga, and Mohammed Bennamoun. 2020. <span>“Hands-on Bayesian Neural Networks–a Tutorial for Deep Learning Users.”</span> <em>arXiv Preprint arXiv:2007.06823</em>.
</div>
<div id="ref-karimi2021algorithmic" class="csl-entry" role="doc-biblioentry">
Karimi, Amir-Hossein, Bernhard Schölkopf, and Isabel Valera. 2021. <span>“Algorithmic Recourse: From Counterfactual Explanations to Interventions.”</span> In <em>Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency</em>, 353–62.
</div>
<div id="ref-karimi2020algorithmic" class="csl-entry" role="doc-biblioentry">
Karimi, Amir-Hossein, Julius Von Kügelgen, Bernhard Schölkopf, and Isabel Valera. 2020. <span>“Algorithmic Recourse Under Imperfect Causal Knowledge: A Probabilistic Approach.”</span> <em>arXiv Preprint arXiv:2006.06831</em>.
</div>
<div id="ref-kehoe2021defence" class="csl-entry" role="doc-biblioentry">
Kehoe, Aidan, Peter Wittek, Yanbo Xue, and Alejandro Pozas-Kerstjens. 2021. <span>“Defence Against Adversarial Attacks Using Classical and Quantum-Enhanced Boltzmann Machines.”</span> <em>Machine Learning: Science and Technology</em>.
</div>
<div id="ref-kilian2017structural" class="csl-entry" role="doc-biblioentry">
Kilian, Lutz, and Helmut Lütkepohl. 2017. <em>Structural Vector Autoregressive Analysis</em>. Cambridge University Press.
</div>
<div id="ref-kirsch2019batchbald" class="csl-entry" role="doc-biblioentry">
Kirsch, Andreas, Joost Van Amersfoort, and Yarin Gal. 2019. <span>“Batchbald: Efficient and Diverse Batch Acquisition for Deep Bayesian Active Learning.”</span> <em>Advances in Neural Information Processing Systems</em> 32: 7026–37.
</div>
<div id="ref-lachapelle2019gradient" class="csl-entry" role="doc-biblioentry">
Lachapelle, Sébastien, Philippe Brouillard, Tristan Deleu, and Simon Lacoste-Julien. 2019. <span>“Gradient-Based Neural Dag Learning.”</span> <em>arXiv Preprint arXiv:1906.02226</em>.
</div>
<div id="ref-lakkaraju2020fool" class="csl-entry" role="doc-biblioentry">
Lakkaraju, Himabindu, and Osbert Bastani. 2020. <span>“" How Do i Fool You?" Manipulating User Trust via Misleading Black Box Explanations.”</span> In <em>Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society</em>, 79–85.
</div>
<div id="ref-lundberg2017unified" class="csl-entry" role="doc-biblioentry">
Lundberg, Scott M, and Su-In Lee. 2017. <span>“A Unified Approach to Interpreting Model Predictions.”</span> In <em>Proceedings of the 31st International Conference on Neural Information Processing Systems</em>, 4768–77.
</div>
<div id="ref-mittelstadt2019explaining" class="csl-entry" role="doc-biblioentry">
Mittelstadt, Brent, Chris Russell, and Sandra Wachter. 2019. <span>“Explaining Explanations in AI.”</span> In <em>Proceedings of the Conference on Fairness, Accountability, and Transparency</em>, 279–88.
</div>
<div id="ref-murphy2022probabilistic" class="csl-entry" role="doc-biblioentry">
Murphy, Kevin P. 2022. <em>Probabilistic Machine Learning: An Introduction</em>. MIT Press.
</div>
<div id="ref-oecd2021artificial" class="csl-entry" role="doc-biblioentry">
OECD. 2021. <span>“Artificial Intelligence, Machine Learning and Big Data in Finance: Opportunities, Challenges and Implications for Policy Makers.”</span> OECD. 2021. <a href="https://www.oecd.org/finance/financial-markets/Artificial-intelligence-machine-learning-big-data-in-finance.pdf">https://www.oecd.org/finance/financial-markets/Artificial-intelligence-machine-learning-big-data-in-finance.pdf</a>.
</div>
<div id="ref-pearl2019seven" class="csl-entry" role="doc-biblioentry">
Pearl, Judea. 2019. <span>“The Seven Tools of Causal Inference, with Reflections on Machine Learning.”</span> <em>Communications of the ACM</em> 62 (3): 54–60.
</div>
<div id="ref-pearl2018book" class="csl-entry" role="doc-biblioentry">
Pearl, Judea, and Dana Mackenzie. 2018. <em>The Book of Why: The New Science of Cause and Effect</em>. Basic books.
</div>
<div id="ref-poyiadzi2020face" class="csl-entry" role="doc-biblioentry">
Poyiadzi, Rafael, Kacper Sokol, Raul Santos-Rodriguez, Tijl De Bie, and Peter Flach. 2020. <span>“FACE: Feasible and Actionable Counterfactual Explanations.”</span> In <em>Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society</em>, 344–50.
</div>
<div id="ref-ribeiro2016should" class="csl-entry" role="doc-biblioentry">
Ribeiro, Marco Tulio, Sameer Singh, and Carlos Guestrin. 2016. <span>“"Why Should i Trust You?" Explaining the Predictions of Any Classifier.”</span> In <em>Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</em>, 1135–44.
</div>
<div id="ref-rudin2019stop" class="csl-entry" role="doc-biblioentry">
Rudin, Cynthia. 2019. <span>“Stop Explaining Black Box Machine Learning Models for High Stakes Decisions and Use Interpretable Models Instead.”</span> <em>Nature Machine Intelligence</em> 1 (5): 206–15.
</div>
<div id="ref-schut2021generating" class="csl-entry" role="doc-biblioentry">
Schut, Lisa, Oscar Key, Rory Mc Grath, Luca Costabello, Bogdan Sacaleanu, Yarin Gal, et al. 2021. <span>“Generating Interpretable Counterfactual Explanations by Implicit Minimisation of Epistemic and Aleatoric Uncertainties.”</span> In <em>International Conference on Artificial Intelligence and Statistics</em>, 1756–64. PMLR.
</div>
<div id="ref-slack2020fooling" class="csl-entry" role="doc-biblioentry">
Slack, Dylan, Sophie Hilgard, Emily Jia, Sameer Singh, and Himabindu Lakkaraju. 2020. <span>“Fooling Lime and Shap: Adversarial Attacks on Post Hoc Explanation Methods.”</span> In <em>Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society</em>, 180–86.
</div>
<div id="ref-szegedy2013intriguing" class="csl-entry" role="doc-biblioentry">
Szegedy, Christian, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, and Rob Fergus. 2013. <span>“Intriguing Properties of Neural Networks.”</span> <em>arXiv Preprint arXiv:1312.6199</em>.
</div>
<div id="ref-ustun2019actionable" class="csl-entry" role="doc-biblioentry">
Ustun, Berk, Alexander Spangher, and Yang Liu. 2019. <span>“Actionable Recourse in Linear Classification.”</span> In <em>Proceedings of the Conference on Fairness, Accountability, and Transparency</em>, 10–19.
</div>
<div id="ref-wachter2017counterfactual" class="csl-entry" role="doc-biblioentry">
Wachter, Sandra, Brent Mittelstadt, and Chris Russell. 2017. <span>“Counterfactual Explanations Without Opening the Black Box: Automated Decisions and the GDPR.”</span> <em>Harv. JL &amp; Tech.</em> 31: 841.
</div>
<div id="ref-zheng2018dags" class="csl-entry" role="doc-biblioentry">
Zheng, Xun, Bryon Aragam, Pradeep Ravikumar, and Eric P Xing. 2018. <span>“Dags with No Tears: Continuous Optimization for Structure Learning.”</span> <em>arXiv Preprint arXiv:1803.01422</em>.
</div>
</div>


</section>
</main> <!-- /main -->
<script type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    setTimeout(function() {
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      let href = ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const cites = ref.parentNode.getAttribute('data-cites').split(' ');
    tippyHover(ref, function() {
      var popup = window.document.createElement('div');
      cites.forEach(function(cite) {
        var citeDiv = window.document.createElement('div');
        citeDiv.classList.add('hanging-indent');
        citeDiv.classList.add('csl-entry');
        var biblioDiv = window.document.getElementById('ref-' + cite);
        if (biblioDiv) {
          citeDiv.innerHTML = biblioDiv.innerHTML;
        }
        popup.appendChild(citeDiv);
      });
      return popup.innerHTML;
    });
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
  </div>
  <div class="nav-page nav-page-next">
  </div>
</nav>
</div> <!-- /content -->


</body></html>