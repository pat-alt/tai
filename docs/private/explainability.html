<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang=""><head>
  <meta charset="utf-8">
  <meta name="generator" content="quarto-0.2.232">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
  <title>Trustworthy AI – explainability</title>
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    span.underline{text-decoration: underline;}
    div.column{display: inline-block; vertical-align: top; width: 50%;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
  </style>

  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
  <script src="../site_libs/quarto-nav/quarto-nav.js"></script>
  <script src="../site_libs/quarto-nav/headroom.min.js"></script>
  <script src="../site_libs/clipboard/clipboard.min.js"></script>
  <meta name="quarto:offset" content="../">
  <script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
  <script src="../site_libs/quarto-search/fuse.min.js"></script>
  <script src="../site_libs/quarto-search/quarto-search.js"></script>
  <script src="../site_libs/quarto-html/quarto.js"></script>
  <script src="../site_libs/quarto-html/popper.min.js"></script>
  <script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
  <script src="../site_libs/quarto-html/anchor.min.js"></script>
  <link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
  <link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet">
  <script src="../site_libs/bootstrap/bootstrap.min.js"></script>
  <link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
  <link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet">
  <script id="quarto-search-options" type="application/json">{
    "location": "navbar",
    "copy-button": false,
    "collapse-after": 2,
    "panel-placement": "end",
    "type": "overlay",
    "limit": 20
  }</script>
  <link rel="stylesheet" href="../styles.css">
</head>
<body>
<div id="quarto-search-results"></div>
<header id="quarto-header" class="headroom fixed-top">
<nav class="navbar navbar-expand-lg navbar-light ">
    <div class="container-fluid">
  <a class="navbar-brand" href="../index.html">
    <span class="navbar-title">Trustworthy AI</span>
  </a>
      <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
      <div class="collapse navbar-collapse" id="navbarCollapse">
        <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../index.html">Home</a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../literature.html">Literature</a>
  </li>  
</ul>
          <div id="quarto-search" class="mb-0 " title="Search">
</div>
      </div> <!-- /navcollapse -->
  </div> <!-- /container-fluid -->
</nav>
</header>
 <!-- /navbar/sidebar -->
<div class="container-fluid quarto-container d-flex flex-column page-layout-article" id="quarto-content">
<div class="row flex-fill">
  <div id="quarto-toc-sidebar" class=" pt-0 col col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-toc order-last"><nav id="TOC" role="doc-toc">
<h2 id="toc-title">On this page</h2>
<ul>
<li><a href="#explainability" class="nav-link active" data-scroll-target="#explainability">Explainability</a>
<ul class="collapse">
<li><a href="#surrogate-explainers" class="nav-link" data-scroll-target="#surrogate-explainers">Surrogate Explainers</a></li>
<li><a href="#criticism-surrogate-explainers" class="nav-link" data-scroll-target="#criticism-surrogate-explainers">Criticism (Surrogate Explainers)</a></li>
<li><a href="#counterfactual-explanations-ce" class="nav-link" data-scroll-target="#counterfactual-explanations-ce">Counterfactual Explanations (CE)</a></li>
</ul></li>
</ul>
<div class="toc-actions"><div><i class="bi bi-github"></i></div><div class="action-links"><p><a href="https://github.com/pat-alt/tai/issues/new" class="toc-action">Report an issue</a></p></div></div></nav></div>
  <div class="col mx-auto col-sm-12 col-md-9 col-lg-8 col-xl-7 px-lg-4 pe-xxl-4 ps-xxl-0">
<main>

<section id="explainability" class="level2">
<h2 class="anchored" data-anchor-id="explainability">Explainability</h2>
<section id="surrogate-explainers" class="level3">
<h3 class="anchored" data-anchor-id="surrogate-explainers">Surrogate Explainers</h3>
<ul>
<li><span class="citation" data-cites="ribeiro2016should">@ribeiro2016should</span> propose Local Interpretable Model-Agnostic Explanations (LIME): the approach involves generating local perturbations in the input space, deriving predictions from the original classifier and than fitting a white box model (e.g.&nbsp;linear regression) on this synthetic data set.</li>
<li><span class="citation" data-cites="lundberg2017unified">@lundberg2017unified</span> propose SHAP as a provably unified approach to additive feature attribution methods (including LIME) with certain desiderata. Contrary to LIME, this approach involves permuting through the feature space and checking how different features impact model predictions when they are included in the permutations.</li>
</ul>
</section>
<section id="criticism-surrogate-explainers" class="level3">
<h3 class="anchored" data-anchor-id="criticism-surrogate-explainers">Criticism (Surrogate Explainers)</h3>
<blockquote class="blockquote">
<p>“Explanatory models by definition do not produce 100% reliable explanations, because they are approximations. This means explanations can’t be fully trusted, and so neither can the original model.” – <a href="https://www.causalens.com/blog/xai-doesnt-explain/">causaLens</a>, 2021</p>
</blockquote>
<ul>
<li><span class="citation" data-cites="mittelstadt2019explaining">@mittelstadt2019explaining</span> points out that there is a gap in the understanding of what explanations are between computer scientists and explanation scientists (social scientists, cognitive scientists, pyschologists, …). Current methods produce at best locally reliable explanations. There needs to be shift towards <em>interactive</em> explanations.</li>
<li><span class="citation" data-cites="rudin2019stop">@rudin2019stop</span> argues that instead of bothering with explanations for black box models we should focus on designing inherently interpretable models. In her view the trade-off between (intrinsic) explainability and performance is not as clear-cut as people claim.</li>
<li><span class="citation" data-cites="lakkaraju2020fool">@lakkaraju2020fool</span> show how misleading black box explanations can manipulate users into trusting an untrustworthy model.</li>
<li><span class="citation" data-cites="slack2020fooling">@slack2020fooling</span> demonstrate that both LIME and SHAP are not reliable: their reliance on feature perturbations makes them susceptible to adversarial attacks.</li>
</ul>
<div class="thoughts">
<ul>
<li>Can we quantify robustness of surrogate explainers?</li>
<li>Comparison of different complexity measures.</li>
<li>Design surrogate explainer that incorporates causality.</li>
<li>Surrogate explainers by definition are approximations of black box models, so can never be 100% trusted.</li>
<li>Adversarially robust surrogate explainers by minimizing divergence between observed and perturbed data. <span class="citation" data-cites="slack2020fooling">[@slack2020fooling]</span></li>
</ul>
</div>
</section>
<section id="counterfactual-explanations-ce" class="level3">
<h3 class="anchored" data-anchor-id="counterfactual-explanations-ce">Counterfactual Explanations (CE)</h3>
<ul>
<li><span class="citation" data-cites="wachter2017counterfactual">@wachter2017counterfactual</span> were among the first to propose counterfactual explanations that do not require knowledge about the inner workings of a black box model.</li>
<li><span class="citation" data-cites="ustun2019actionable">@ustun2019actionable</span> propose a framework for <em>actionable</em> recourse in the context of linear classifiers.</li>
<li><span class="citation" data-cites="joshi2019towards">@joshi2019towards</span> extend the framework of <span class="citation" data-cites="ustun2019actionable">@ustun2019actionable</span>. Their proposed REVISE method is applicable to a broader class of models including black box classifiers and structural causal models. For a summary see <a href="https://pat-alt.github.io/2021/04/27/individual-recourse-for-black-box-models/">here</a> and for a set of slides see <a href="https://pat-alt.github.io/2021/04/27/individual-recourse-for-black-box-models/paper_presentation.pdf">here</a>.</li>
<li><span class="citation" data-cites="poyiadzi2020face">@poyiadzi2020face</span> propose FACE: feasible and actionable counterfactual explanations. The premise is that the shortest distance to the decision boundary may not be a desirable counterfactual.</li>
<li><span class="citation" data-cites="schut2021generating">@schut2021generating</span> introduce Bayesian modelling to the context of CE: their approach implicitly minimizes <em>aleatoric</em> and <em>epistemic</em> uncertainty to generate a CE that us <em>unambiguous</em> and <em>realistic</em>, respectively.</li>
</ul>
<div class="thoughts">
<ul>
<li><strong>Test what counterfactual explanations are most desirable through user study at ING</strong></li>
<li>Design counterfactual explanations that incorporate causality
<ul>
<li><strong>There are in fact several very recent papers (including <span class="citation" data-cites="joshi2019towards">@joshi2019towards</span>) that link CGMs to counterfactual explanations</strong> (see below)</li>
</ul></li>
<li>Time series: what is the link to counterfactual analysis in multivariate time series? (e.g.&nbsp;chapter 4 in <span class="citation" data-cites="kilian2017structural">@kilian2017structural</span>)</li>
<li>What about the continuous outcome variables? (e.g.&nbsp;target inflation rate, … ING cases?)</li>
<li>How do counterfactual explainers fare where LIME/SHAP fail? <span class="citation" data-cites="slack2020fooling">[@slack2020fooling]</span></li>
<li>Can counterfactual explainers be fooled much like LIME/SHAP?</li>
<li>Can we establish a link between counterfactual and surrogate explainers? Important attributes identified by LIME/SHAP should play a prominent role in counterfactuals.</li>
<li>Can counterfactual explainers be used to detect adversarial examples?</li>
<li>Limiting behaviour: what happens if all individual with negative outcome move across the original decision boundary?</li>
</ul>
</div>


</section>
</section>
<script type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    setTimeout(function() {
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    window.tippy(el, {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    }); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      let href = ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const cites = ref.parentNode.getAttribute('data-cites').split(' ');
    tippyHover(ref, function() {
      var popup = window.document.createElement('div');
      cites.forEach(function(cite) {
        var citeDiv = window.document.createElement('div');
        citeDiv.classList.add('hanging-indent');
        citeDiv.classList.add('csl-entry');
        var biblioDiv = window.document.getElementById('ref-' + cite);
        if (biblioDiv) {
          citeDiv.innerHTML = biblioDiv.innerHTML;
        }
        popup.appendChild(citeDiv);
      });
      return popup.innerHTML;
    });
  }
});
</script>
</main>
<div class="page-navigation ">
  <div class="nav-page nav-page-previous">
  </div>
  <div class="nav-page nav-page-next">
  </div>
</div>
</div> <!-- /main column -->
</div> <!-- /row -->
<div class="row">
  <div class="nav-footer">
      <div class="nav-footer-center me-auto ms-auto">Copyright 2021, <a href="https://www.paltmeyer.com/">Patrick Altmeyer</a></div>
  </div>
</div>
</div> <!-- /container fluid -->


</body></html>